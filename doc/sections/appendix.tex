\documentclass[../main.tex]{subfiles}

\begin{document}
% ------------------------------------------------
\section{Appendix}
\subsection{Source code}
All the source code is located in \href{https://github.com/kmaasrud/reg-resample-fys-stk4155}{this GitHub repository}.

\subsection{$L_0$ regression on matrix form}
\label{sec:L0_matrix_form}
The cost function we use for OLS regression is the residual sum of squares (RSS) function:

\begin{equation*}
  \text{RSS}(\beta) = \sum_{i=1}^N \left(y_i - \hat y_i\right)^2 .
\end{equation*}

Changing into matrix notation, we get

\begin{equation*}
  \text{RSS}(\beta) = (\mathbf y - \hat{\mathbf y})^T(\mathbf y - \hat{\mathbf y}) = (\mathbf y  - \mathbf X\beta)^T(\mathbf y - \mathbf X\beta) ,
\end{equation*}

which we can differentiate with respect to $\beta$ to find the minimum.

\begin{equation*}
  \frac{\partial \text{RSS}}{\partial \beta} = -2\mathbf X^T (\mathbf y - \mathbf X\beta) .
\end{equation*}

Assuming full column rank for $\mathbf X$, $(\mathbf X^T \mathbf X)$ is thus positive definite (and importantly, invertible). Setting the first derivative to $0$, we get

\begin{equation*}
  \mathbf X^T(\mathbf y - \mathbf X\beta) = 0
\end{equation*}

\begin{equation*}
  \Rightarrow \hat \beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf y
\end{equation*}


% ------------------------------------------------
\subsection{Deriving the bias-variance decomposition}\label{sec:bv_decomp}
The cost fuction is defined as

\begin{align*}
    C(\mathbf{X}, \boldsymbol{\beta}) = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2=\mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right].
    %\label{eq:cost-function}
\end{align*} We want to decompose the cost function so that it can be expressed as the sum of bias, variance and the standard deviation. 

The first step is to substitute \ensuremath{\mathbf{y}=f(\mathbf{x})+\boldsymbol{\epsilon}} into the cost function, and add and subtract the expectation value of the estimator variable \ensuremath{\mathbb{E}[\mathbf{\tilde{y}}]}. We get 

\begin{align*}
    \mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}}+\mathbb{E}[\mathbf{\tilde{y}}]-\mathbb{E}[\mathbf{\tilde{y}}])^2\right] = \sum_{i}\mathbb{E}\left[((\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])+\mathbf{\epsilon}+(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}}))^2\right]. 
\end{align*} By writing out this expression and using that \ensuremath{\mathbb{E}[\mathbf{\epsilon}]=0} and \ensuremath{\mathbb{E}[\mathbf{f}]=f} we get

\begin{align*}
    \mathbb{E}\left[(\mathbf{f}+\mathbf{\epsilon}-\mathbf{\tilde{y}}+\mathbb{E}[\mathbf{\tilde{y}}]-\mathbb{E}[\mathbf{\tilde{y}}])^2\right] &= 
    \mathbb{E}[(\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])]+\mathbb{E}[\mathbf{\epsilon^2}]+\mathbb{E}[\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}}]\\&+\underbrace{2\mathbb{E}[\boldsymbol{\epsilon}(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}})]}_{\text{=0}}+\underbrace{2\mathbb{E}[\boldsymbol{\epsilon}(\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])]}_{\text{=0}}\\&+\underbrace{\mathbb{E}[(\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])(\mathbb{E}[\mathbf{\tilde{y}}-\mathbf{\tilde{y}})]}_{\text{=0}}
\end{align*} Thus, we are left with 

\begin{align*}
    (\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])^2+\mathbb{E}[\boldsymbol{\epsilon^2}]+\mathbb{E}[(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}})^2].
\end{align*}

The variance of the response variable is given by
\begin{align*}
    \mathrm{Var}(\mathbf{f}+\boldsymbol{\epsilon})&=\mathbb{E}[(\mathbf{f}+\boldsymbol{\epsilon})^2]-\mathbb{E}[\mathbf{f}+\boldsymbol{\epsilon}]\\&=\mathbb{E}[\mathbf{y}]^2+\underbrace{2\mathbb{E}[\mathbf{f}]\mathbb{E}[\boldsymbol{\epsilon}]}_{=0}+\mathbb{E}[\boldsymbol{\epsilon}^2]+(\underbrace{\mathbb{E}[\boldsymbol{\epsilon}]}_{=0}+\mathbb{E}[\mathbf{f}])^2
\end{align*} We see that 

\begin{align*}
    \mathrm{Var}(\mathbf{f}+\boldsymbol{\epsilon})=\mathbb{E}[\boldsymbol{\epsilon}^2]=\sigma^2.
\end{align*} The bias is defined as \ensuremath{(\mathbf{f}-\mathbb{E}[\mathbf{\tilde{y}}])^2=\mathrm{Bias}(\mathbf{\tilde{y}})}. \ensuremath{\mathbb{E}[(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}})^2]} can be rewritten as

\begin{align*}
    \mathbb{E}[(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}})^2] &=\mathbb{E}[\mathbb{E}[\mathbf{\tilde{y}}]]-2\mathbb{E}[\mathbf{\tilde{y}}]\mathbb{E}[\mathbb{E}[\mathbf{\tilde{y}}]+\mathbb{E}[\mathbf{\tilde{y}}]^2]\\
    &=\mathbb{E}[\mathbf{\tilde{y}}^2]-\mathbb{E}[\mathbf{\tilde{y}}]^2\\
    &=\mathrm{Var}(\mathbf{\tilde{y}})
\end{align*} By discretizing this equation the expression above can be written as

\begin{align*}
    \mathbb{E}[(\mathbb{E}[\mathbf{\tilde{y}}]-\mathbf{\tilde{y}})^2]=\mathrm{Var}(\mathbf{\tilde{y}})=\frac{1}{n}\sum_{i=0}^{n-1}(\tilde{y}_i-\mathbb{E}[\mathbf{\tilde{y}})^2]
\end{align*}

Finally, we obtain the result

\begin{align*}
    \mathbb{E}\left[(\mathbf{y}-\mathbf{\tilde{y}})^2\right]&=\frac{1}{n}\sum_i(f_i-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2+\frac{1}{n}\sum_i(\tilde{y}_i-\mathbb{E}\left[\mathbf{\tilde{y}}\right])^2+\sigma^2\\
    &=\mathrm{Bias}[\mathbf{\tilde{y}}]^2+\sigma^2+\mathrm{Var}[\mathbf{\tilde{y}}].
\end{align*}

\end{document}
