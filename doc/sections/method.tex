\documentclass[../main.tex]{subfiles}
\begin{document}

\section{Method and implementation}\label{sec:method}
The implementation of the code is pretty straight forward, having the regression models in a separate class, for easy use and access. Everything that has something to do with franke's function, for example plotting of the function or accumulation of data from the function is collected in a python file "franke". The resampling techniques is also gathered in a separate python file containing functions used for bootstrap, k-folds and cross validation in the python file "assessment". The last python file worth mentioning is "utils" which is a collection of a variety of different functions that is handy during the entire project, including a mean squared error function, $R^2$ score function, a function which makes a design matrix, a single value decomposition function and a bit other convenient functions.

\subsection{The design matrix}
To create the design matrix, the x and y arrays are sent into a function together with the desired degree of polynomial. Then the function checks the lengths of the arrays and the degree wanted, and uses this to calculate the dimensions of the design matrix needed to have the matrix on the form like the Vandermonde matrix discussed in the theory section. Then a double for loop is used to fill an identity matrix of the desired dimensions with the prefered x and y combinations to represent the right polynomials.

\subsubsection{Input- design matrix}
It is natural to start wondering how the x and y data that is sent into the design matrix function is produced. This is quiet easy done by making arrays of the size N=20 of sizes between between 0 and 1, and applying frankes function to the data points.

\subsection{Regression methods}
\subsubsection{Ridge}
To explain the implementation of the regression methods, it makes more sense to explain the ridge class first. The implemented ridge class starts of by defining different variables like the design matrix X, the dimensions n and p, y, lambdas lmb and the betas.\\

Then the ridge coefficient vector is calculated the way explained in the theory section (\ref{rid}), which is easier to understand by seeing the code
\begin{minted}{python}
        # Matrix inversion to find beta
        return np.linalg.inv(self.X.T @ self.X + self.lmb*I) @ self.X.T @ self.y
\end{minted}
Where the linalg package is being use to invert the matrix. After this the prediction is calculated by taking the dot product of the design matrix and the calculated betas.

\subsubsection{Ordinary least squares}
The OLS method is computed by using the ridge class, which was the reason the ridge class was explained first. The difference between the implementations is that in OLS $\lambda=0$. Which makes it quiet straight forward to call the class and accessing the methods.

\subsubsection{Lasso}
Using the Lasso regression method the implementation was done by using scikit-learn, which is a python library with various machine learning algorithms. The method was performed by the following code
\begin{minted}{python}
    def fit_beta(self):
        sklearn_lasso = linear_model.Lasso(alpha=self.lmb)
        sklearn_lasso.fit(self.X, self.y)
        return sklearn_lasso.coef_
\end{minted}
where the self.X is the design matrix and and self.y is the response.

\end{document}
